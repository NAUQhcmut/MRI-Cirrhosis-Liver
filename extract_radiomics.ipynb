{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13222468,"sourceType":"datasetVersion","datasetId":8381149},{"sourceId":13768750,"sourceType":"datasetVersion","datasetId":8762910},{"sourceId":13769001,"sourceType":"datasetVersion","datasetId":8763121},{"sourceId":13769603,"sourceType":"datasetVersion","datasetId":8763540},{"sourceId":13970629,"sourceType":"datasetVersion","datasetId":8906390}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pyradiomics\nimport yaml\nfrom radiomics import featureextractor\nimport pandas as pd\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-11T03:39:13.681918Z","iopub.execute_input":"2026-01-11T03:39:13.682172Z","iopub.status.idle":"2026-01-11T03:39:35.129698Z","shell.execute_reply.started":"2026-01-11T03:39:13.682156Z","shell.execute_reply":"2026-01-11T03:39:35.129016Z"}},"outputs":[{"name":"stdout","text":"Collecting pyradiomics\n  Downloading pyradiomics-3.1.0.tar.gz (34.5 MB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/03/c1/20fc2c50ab1e3304da36d866042a1905a2b05a1431ece35448ab6b4578f2/pyradiomics-3.1.0.tar.gz (from https://pypi.org/simple/pyradiomics/)\u001b[0m: \u001b[33mRequested pyradiomics from https://files.pythonhosted.org/packages/03/c1/20fc2c50ab1e3304da36d866042a1905a2b05a1431ece35448ab6b4578f2/pyradiomics-3.1.0.tar.gz has inconsistent version: expected '3.1.0', but metadata has '3.0.1a1'\u001b[0m\n  Downloading pyradiomics-3.0.1.tar.gz (34.5 MB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.11/dist-packages (from pyradiomics) (1.26.4)\nRequirement already satisfied: SimpleITK>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from pyradiomics) (2.5.2)\nRequirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pyradiomics) (1.8.0)\nCollecting pykwalify>=1.6.0 (from pyradiomics)\n  Downloading pykwalify-1.8.0-py2.py3-none-any.whl.metadata (5.3 kB)\nRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from pyradiomics) (1.17.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.9.2->pyradiomics) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.9.2->pyradiomics) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.9.2->pyradiomics) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.9.2->pyradiomics) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.9.2->pyradiomics) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.9.2->pyradiomics) (2.4.1)\nCollecting docopt>=0.6.2 (from pykwalify>=1.6.0->pyradiomics)\n  Downloading docopt-0.6.2.tar.gz (25 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from pykwalify>=1.6.0->pyradiomics) (2.9.0.post0)\nCollecting ruamel.yaml>=0.16.0 (from pykwalify>=1.6.0->pyradiomics)\n  Downloading ruamel_yaml-0.19.1-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.9.2->pyradiomics) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.9.2->pyradiomics) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.9.2->pyradiomics) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.9.2->pyradiomics) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.9.2->pyradiomics) (2024.2.0)\nDownloading pykwalify-1.8.0-py2.py3-none-any.whl (24 kB)\nDownloading ruamel_yaml-0.19.1-py3-none-any.whl (118 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m118.1/118.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: pyradiomics, docopt\n  Building wheel for pyradiomics (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for pyradiomics: filename=pyradiomics-3.0.1-cp311-cp311-linux_x86_64.whl size=169811 sha256=dd7762278cd112c7b56c548575872e9016b27951d343d44c3fe2bd4ddb334d04\n  Stored in directory: /root/.cache/pip/wheels/57/fb/95/931a5364368a3d8eaf9eee0daaf186e64b75fea7c1c97949fb\n  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=3d15e0744b254a512e879f5d3d8614783b73cfee1ab0f72fb546a1c43e91fd61\n  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\nSuccessfully built pyradiomics docopt\nInstalling collected packages: docopt, ruamel.yaml, pykwalify, pyradiomics\nSuccessfully installed docopt-0.6.2 pykwalify-1.8.0 pyradiomics-3.0.1 ruamel.yaml-0.19.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"\n# G·ªëc dataset\nbase_dir = \"/kaggle/input/t1-mri-scan/Cirrhosis_T1_3D\"\n\n# C·∫•u tr√∫c folder\nsplits = {\n    \"train\": {\"images\": \"train_images\", \"masks\": \"train_masks\"},\n    \"test\": {\"images\": \"test_images\", \"masks\": \"test_masks\"},\n    \"valid\": {\"images\": \"valid_images\", \"masks\": \"valid_masks\"}\n}\n\nrows = []\n\nfor split, dirs in splits.items():\n    img_dir = os.path.join(base_dir, dirs[\"images\"])\n    mask_dir = os.path.join(base_dir, dirs[\"masks\"])\n\n    # duy·ªát qua to√†n b·ªô file trong folder ·∫£nh\n    for f in os.listdir(img_dir):\n        if f.endswith(\".nii.gz\") or f.endswith(\".nii\"):  # nh·∫≠n c·∫£ .nii v√† .nii.gz\n            patient_id = os.path.splitext(os.path.splitext(f)[0])[0]\n            img_path = os.path.join(img_dir, f)\n            mask_path = os.path.join(mask_dir, f)\n\n            if os.path.exists(mask_path):\n                rows.append({\n                    \"PatientID\": patient_id,\n                    \"Image\": img_path,\n                    \"Mask\": mask_path,\n                    \"Split\": split\n                })\n            else:\n                print(f\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y mask cho {f} ({split})\")\n\n# Xu·∫•t CSV\ndf = pd.DataFrame(rows)\ndf.to_csv(\"dataset_1.csv\", index=False)\n\nprint(f\"‚úÖ T·∫°o file dataset.csv v·ªõi {len(df)} d√≤ng\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-11T03:45:06.645265Z","iopub.execute_input":"2026-01-11T03:45:06.645724Z","iopub.status.idle":"2026-01-11T03:45:07.214947Z","shell.execute_reply.started":"2026-01-11T03:45:06.645697Z","shell.execute_reply":"2026-01-11T03:45:07.214324Z"}},"outputs":[{"name":"stdout","text":"‚úÖ T·∫°o file dataset.csv v·ªõi 310 d√≤ng\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"\n# G·ªëc dataset\nbase_dir = \"/kaggle/input/t2-mri-scan/Cirrhosis_T2_3D\"\n\n# C·∫•u tr√∫c folder\nsplits = {\n    \"train\": {\"images\": \"train_images\", \"masks\": \"train_masks\"},\n    \"test\": {\"images\": \"test_images\", \"masks\": \"test_masks\"},\n    \"valid\": {\"images\": \"valid_images\", \"masks\": \"valid_masks\"}\n}\n\nrows = []\n\nfor split, dirs in splits.items():\n    img_dir = os.path.join(base_dir, dirs[\"images\"])\n    mask_dir = os.path.join(base_dir, dirs[\"masks\"])\n\n    # duy·ªát qua to√†n b·ªô file trong folder ·∫£nh\n    for f in os.listdir(img_dir):\n        if f.endswith(\".nii.gz\") or f.endswith(\".nii\"):  # nh·∫≠n c·∫£ .nii v√† .nii.gz\n            patient_id = os.path.splitext(os.path.splitext(f)[0])[0]\n            img_path = os.path.join(img_dir, f)\n            mask_path = os.path.join(mask_dir, f)\n\n            if os.path.exists(mask_path):\n                rows.append({\n                    \"PatientID\": patient_id,\n                    \"Image\": img_path,\n                    \"Mask\": mask_path,\n                    \"Split\": split\n                })\n            else:\n                print(f\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y mask cho {f} ({split})\")\n\n# Xu·∫•t CSV\ndf = pd.DataFrame(rows)\ndf.to_csv(\"dataset_2.csv\", index=False)\n\nprint(f\"‚úÖ T·∫°o file dataset.csv v·ªõi {len(df)} d√≤ng\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-11T03:45:20.462005Z","iopub.execute_input":"2026-01-11T03:45:20.462263Z","iopub.status.idle":"2026-01-11T03:45:20.950307Z","shell.execute_reply.started":"2026-01-11T03:45:20.462244Z","shell.execute_reply":"2026-01-11T03:45:20.949719Z"}},"outputs":[{"name":"stdout","text":"‚úÖ T·∫°o file dataset.csv v·ªõi 318 d√≤ng\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"config_dict = {\n    \"imageType\": {\n        \"Original\": {},\n        \"Wavelet\": {},\n        \"LoG\": {\"sigma\": [1.0, 2.0, 3.0]}  # L·ªçc Gaussian Laplace\n    },\n    \"featureClass\": {\n        \"shape\": None,\n        \"firstorder\": None,\n        \"glcm\": None,\n        \"glrlm\": None,\n        \"glszm\": None,\n        \"gldm\": None,\n        \"ngtdm\": None\n    },\n    \"setting\": {\n        \"resampledPixelSpacing\": [1, 1, 1],  # Chu·∫©n h√≥a voxel spacing\n        \"interpolator\": \"sitkBSpline\",\n        \"binWidth\": 25,         # Quantization bin width\n        \"padDistance\": 5,\n        \"correctMask\": True\n    },\n}\n# L∆∞u config\nwith open(\"config_mri.yaml\", \"w\") as f:\n    yaml.dump(config_dict, f, default_flow_style=False)\nextractor = featureextractor.RadiomicsFeatureExtractor(\"config_mri.yaml\")\ndf = pd.read_csv(\"dataset_1.csv\")\nall_features = []\nfor idx, row in df.iterrows():\n    try:\n        features = extractor.execute(row[\"Image\"], row[\"Mask\"])\n        # Gi·ªØ l·∫°i c√°c feature b·∫Øt ƒë·∫ßu b·∫±ng \"original\"\n        feature_dict = {k: v for k, v in features.items() if k.startswith(\"original\")}\n        # Th√™m th√¥ng tin ID, Split\n        feature_dict[\"PatientID\"] = row[\"PatientID\"]\n        feature_dict[\"Split\"] = row[\"Split\"]\n        all_features.append(feature_dict)\n        print(f\"‚úÖ Done {row['PatientID']} ({row['Split']})\")\n    except Exception as e:\n        print(f\"‚ùå Error {row['PatientID']}: {e}\")\nfeatures_df = pd.DataFrame(all_features)\nfeatures_df.to_csv(\"features_mri_1.csv\", index=False)\nprint(\"üéâ Features saved to features_mri_1.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"config_dict = {\n    \"imageType\": {\n        \"Original\": {},\n        \"Wavelet\": {},\n        \"LoG\": {\"sigma\": [1.0, 2.0, 3.0]}  # L·ªçc Gaussian Laplace\n    },\n    \"featureClass\": {\n        \"shape\": None,\n        \"firstorder\": None,\n        \"glcm\": None,\n        \"glrlm\": None,\n        \"glszm\": None,\n        \"gldm\": None,\n        \"ngtdm\": None\n    },\n    \"setting\": {\n        \"resampledPixelSpacing\": [1, 1, 1],  # Chu·∫©n h√≥a voxel spacing\n        \"interpolator\": \"sitkBSpline\",\n        \"binWidth\": 25,         # Quantization bin width\n        \"padDistance\": 5,\n        \"correctMask\": True\n    },\n}\n# L∆∞u config\nwith open(\"config_mri.yaml\", \"w\") as f:\n    yaml.dump(config_dict, f, default_flow_style=False)\nextractor = featureextractor.RadiomicsFeatureExtractor(\"config_mri.yaml\")\ndf = pd.read_csv(\"dataset_2.csv\")\nall_features = []\nfor idx, row in df.iterrows():\n    try:\n        features = extractor.execute(row[\"Image\"], row[\"Mask\"])\n        # Gi·ªØ l·∫°i c√°c feature b·∫Øt ƒë·∫ßu b·∫±ng \"original\"\n        feature_dict = {k: v for k, v in features.items() if k.startswith(\"original\")}\n        # Th√™m th√¥ng tin ID, Split\n        feature_dict[\"PatientID\"] = row[\"PatientID\"]\n        feature_dict[\"Split\"] = row[\"Split\"]\n        all_features.append(feature_dict)\n        print(f\"‚úÖ Done {row['PatientID']} ({row['Split']})\")\n    except Exception as e:\n        print(f\"‚ùå Error {row['PatientID']}: {e}\")\nfeatures_df = pd.DataFrame(all_features)\nfeatures_df.to_csv(\"features_mri_2.csv\", index=False)\nprint(\"üéâ Features saved to features_mri_2.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\n\n# Th∆∞ m·ª•c g·ªëc dataset\nbase_dir = \"/kaggle/input/healthy/Healthy_subjects/T1_W_Healthy\"\n\n# ƒê·ªãnh nghƒ©a th∆∞ m·ª•c T1 images v√† T1 masks\nimg_dir = os.path.join(base_dir, \"T1_images\")\nmask_dir = os.path.join(base_dir, \"T1_masks\")\n\nrows = []\n\n# Duy·ªát to√†n b·ªô ·∫£nh trong folder T1_images\nfor f in os.listdir(img_dir):\n    if f.endswith(\".nii.gz\") or f.endswith(\".nii\"):\n        patient_id = os.path.splitext(os.path.splitext(f)[0])[0]\n        img_path = os.path.join(img_dir, f)\n        mask_path = os.path.join(mask_dir, f)\n\n        if os.path.exists(mask_path):\n            rows.append({\n                \"PatientID\": patient_id,\n                \"Image\": img_path,\n                \"Mask\": mask_path\n            })\n        else:\n            print(f\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y mask cho {f}\")\n\n# Xu·∫•t CSV\ndf = pd.DataFrame(rows)\ndf.to_csv(\"dataset_T1_healthy.csv\", index=False)\n\nprint(f\"‚úÖ T·∫°o file dataset_T1.csv v·ªõi {len(df)} d√≤ng\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-11T03:45:47.717061Z","iopub.execute_input":"2026-01-11T03:45:47.717342Z","iopub.status.idle":"2026-01-11T03:45:47.814780Z","shell.execute_reply.started":"2026-01-11T03:45:47.717321Z","shell.execute_reply":"2026-01-11T03:45:47.814167Z"}},"outputs":[{"name":"stdout","text":"‚úÖ T·∫°o file dataset_T1.csv v·ªõi 55 d√≤ng\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import os\nimport pandas as pd\n\n# Th∆∞ m·ª•c g·ªëc dataset\nbase_dir = \"/kaggle/input/healthy/Healthy_subjects/T2_W_Healthy\"\n\n# ƒê·ªãnh nghƒ©a th∆∞ m·ª•c T1 images v√† T1 masks\nimg_dir = os.path.join(base_dir, \"T2_images\")\nmask_dir = os.path.join(base_dir, \"T2_masks\")\n\nrows = []\n\n# Duy·ªát to√†n b·ªô ·∫£nh trong folder T1_images\nfor f in os.listdir(img_dir):\n    if f.endswith(\".nii.gz\") or f.endswith(\".nii\"):\n        patient_id = os.path.splitext(os.path.splitext(f)[0])[0]\n        img_path = os.path.join(img_dir, f)\n        mask_path = os.path.join(mask_dir, f)\n\n        if os.path.exists(mask_path):\n            rows.append({\n                \"PatientID\": patient_id,\n                \"Image\": img_path,\n                \"Mask\": mask_path\n            })\n        else:\n            print(f\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y mask cho {f}\")\n\n# Xu·∫•t CSV\ndf = pd.DataFrame(rows)\ndf.to_csv(\"dataset_T2_healthy.csv\", index=False)\n\nprint(f\"‚úÖ T·∫°o file dataset_T2.csv v·ªõi {len(df)} d√≤ng\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-11T03:45:51.330140Z","iopub.execute_input":"2026-01-11T03:45:51.330883Z","iopub.status.idle":"2026-01-11T03:45:51.456577Z","shell.execute_reply.started":"2026-01-11T03:45:51.330859Z","shell.execute_reply":"2026-01-11T03:45:51.455922Z"}},"outputs":[{"name":"stdout","text":"‚úÖ T·∫°o file dataset_T2.csv v·ªõi 55 d√≤ng\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"config_dict = {\n    \"imageType\": {\n        \"Original\": {},\n        \"Wavelet\": {},\n        \"LoG\": {\"sigma\": [1.0, 2.0, 3.0]}  # L·ªçc Gaussian Laplace\n    },\n    \"featureClass\": {\n        \"shape\": None,\n        \"firstorder\": None,\n        \"glcm\": None,\n        \"glrlm\": None,\n        \"glszm\": None,\n        \"gldm\": None,\n        \"ngtdm\": None\n    },\n    \"setting\": {\n        \"resampledPixelSpacing\": [1, 1, 1],  # Chu·∫©n h√≥a voxel spacing\n        \"interpolator\": \"sitkBSpline\",\n        \"binWidth\": 25,         # Quantization bin width\n        \"padDistance\": 5,\n        \"correctMask\": True\n    },\n}\n# L∆∞u config\nwith open(\"config_mri.yaml\", \"w\") as f:\n    yaml.dump(config_dict, f, default_flow_style=False)\nextractor = featureextractor.RadiomicsFeatureExtractor(\"config_mri.yaml\")\ndf = pd.read_csv(\"dataset_T1_healthy.csv\")\nall_features = []\nfor idx, row in df.iterrows():\n    try:\n        features = extractor.execute(row[\"Image\"], row[\"Mask\"])\n        # Gi·ªØ l·∫°i c√°c feature b·∫Øt ƒë·∫ßu b·∫±ng \"original\"\n        feature_dict = {k: v for k, v in features.items() if k.startswith(\"original\")}\n        # Th√™m th√¥ng tin ID, Split\n        feature_dict[\"PatientID\"] = row[\"PatientID\"]\n        all_features.append(feature_dict)\n        print(f\"‚úÖ Done {row['PatientID']}\")\n    except Exception as e:\n        print(f\"‚ùå Error {row['PatientID']}: {e}\")\nfeatures_df = pd.DataFrame(all_features)\nfeatures_df.to_csv(\"features_mri_healthy_T1.csv\", index=False)\nprint(\"üéâ Features saved to features_mri_healthy.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"config_dict = {\n    \"imageType\": {\n        \"Original\": {},\n        \"Wavelet\": {},\n        \"LoG\": {\"sigma\": [1.0, 2.0, 3.0]}  # L·ªçc Gaussian Laplace\n    },\n    \"featureClass\": {\n        \"shape\": None,\n        \"firstorder\": None,\n        \"glcm\": None,\n        \"glrlm\": None,\n        \"glszm\": None,\n        \"gldm\": None,\n        \"ngtdm\": None\n    },\n    \"setting\": {\n        \"resampledPixelSpacing\": [1, 1, 1],  # Chu·∫©n h√≥a voxel spacing\n        \"interpolator\": \"sitkBSpline\",\n        \"binWidth\": 25,         # Quantization bin width\n        \"padDistance\": 5,\n        \"correctMask\": True\n    },\n}\n# L∆∞u config\nwith open(\"config_mri.yaml\", \"w\") as f:\n    yaml.dump(config_dict, f, default_flow_style=False)\nextractor = featureextractor.RadiomicsFeatureExtractor(\"config_mri.yaml\")\ndf = pd.read_csv(\"dataset_T2_healthy.csv\")\nall_features = []\nfor idx, row in df.iterrows():\n    try:\n        features = extractor.execute(row[\"Image\"], row[\"Mask\"])\n        # Gi·ªØ l·∫°i c√°c feature b·∫Øt ƒë·∫ßu b·∫±ng \"original\"\n        feature_dict = {k: v for k, v in features.items() if k.startswith(\"original\")}\n        # Th√™m th√¥ng tin ID, Split\n        feature_dict[\"PatientID\"] = row[\"PatientID\"]\n        all_features.append(feature_dict)\n        print(f\"‚úÖ Done {row['PatientID']}\")\n    except Exception as e:\n        print(f\"‚ùå Error {row['PatientID']}: {e}\")\nfeatures_df = pd.DataFrame(all_features)\nfeatures_df.to_csv(\"features_mri_healthy_T2.csv\", index=False)\nprint(\"üéâ Features saved to features_mri_healthy.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# N√©n folder working th√†nh zip\n!zip -r results.zip /kaggle/working/\n\n# T·∫°o link t·∫£i\nfrom IPython.display import FileLink\nFileLink(\"results.zip\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", device)\n\n# N·∫øu GPU OK s·∫Ω in: Device: cuda\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport torchvision.transforms as T\nfrom torchvision.models import resnet50\nfrom PIL import Image\nimport pandas as pd\nfrom tqdm import tqdm\n\n\n# ---------------------------\n# 1. RESNET50 PRETRAINED\n# ---------------------------\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nmodel = resnet50(weights=\"IMAGENET1K_V1\")\nmodel = torch.nn.Sequential(*list(model.children())[:-1])  # b·ªè FC ‚Üí embedding 2048D\nmodel.to(device)\nmodel.eval()\n\n# ---------------------------\n# 2. IMAGE TRANSFORM\n# ---------------------------\ntransform = T.Compose([\n    T.Resize((224, 224)),\n    T.ToTensor(),\n    T.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225],\n    ),\n])\n\n\n# ---------------------------\n# 3. EXTRACT FEATURE FROM 1 IMAGE\n# ---------------------------\ndef extract_deep_feature(img_path):\n    img = Image.open(img_path).convert(\"RGB\")\n    x = transform(img).unsqueeze(0).to(device)\n    with torch.no_grad():\n        feat = model(x).squeeze().cpu().numpy()\n    return feat\n\n\n# ---------------------------\n# 4. MAIN EXTRACTION LOOP\n# ---------------------------\ndef extract_from_dataset(base_dir,x):\n\n    rows = []\n    splits = [\"train\", \"valid\", \"test\"]\n\n    for split in splits:\n        img_root = os.path.join(base_dir, f\"{split}_images\")\n\n        if not os.path.exists(img_root):\n            print(f\"‚ùå Folder kh√¥ng t·ªìn t·∫°i: {img_root}\")\n            continue\n\n        # m·ªói folder b√™n trong = 1 b·ªánh nh√¢n\n        patients = [p for p in os.listdir(img_root) \n                    if os.path.isdir(os.path.join(img_root, p))]\n\n        for patient_id in tqdm(patients, desc=f\"Split {split}\"):\n\n            patient_folder = os.path.join(img_root, patient_id)\n\n            image_files = [\n                f for f in os.listdir(patient_folder)\n                if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n            ]\n\n            for img_file in image_files:\n\n                img_path = os.path.join(patient_folder, img_file)\n\n                feat = extract_deep_feature(img_path)\n\n                rows.append({\n                    \"Split\": split,\n                    \"PatientID\": patient_id,\n                    \"ImageFile\": img_file,\n                    **{f\"F{i}\": feat[i] for i in range(len(feat))}\n                })\n\n    df = pd.DataFrame(rows)\n    df.to_csv(f\"deep_features_2d_{x}.csv\", index=False)\n    print(f\"‚úÖ Saved deep_features_2d_{x}.csv\")\n\n    return df\n\n\n# RUN:\ndf1 = extract_from_dataset(\"/kaggle/input/2dpicture/Cirrhosis_T1_3D\",1)\ndf2 = extract_from_dataset(\"/kaggle/input/2dpicture/Cirrhosis_T2_3D\",2)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# Load file deep feature c·ªßa b·∫°n\n# L·∫•y danh s√°ch c·ªôt feature F0..F2047\nfeature_cols = [c for c in df1.columns if c.startswith(\"F\")]\n\n# Group theo b·ªánh nh√¢n\npatient_df = (\n    df1.groupby([\"Split\", \"PatientID\"])[feature_cols]\n      .mean()     # mean pooling\n      .reset_index()\n)\n\n# L∆∞u file\npatient_df.to_csv(\"deep_features_patient_T1.csv\", index=False)\n\nprint(\"‚úÖ Saved deep_features_patient_T1.csv\")\nprint(\"S·ªë b·ªánh nh√¢n:\", len(patient_df))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# Load file deep feature c·ªßa b·∫°n\ndf=df2\n# L·∫•y danh s√°ch c·ªôt feature F0..F2047\nfeature_cols = [c for c in df.columns if c.startswith(\"F\")]\n\n# Group theo b·ªánh nh√¢n\npatient_df = (\n    df.groupby([\"Split\", \"PatientID\"])[feature_cols]\n      .mean()     # mean pooling\n      .reset_index()\n)\n\n# L∆∞u file\npatient_df.to_csv(\"deep_features_patient_T2.csv\", index=False)\n\nprint(\"‚úÖ Saved deep_features_patient_T2.csv\")\nprint(\"S·ªë b·ªánh nh√¢n:\", len(patient_df))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport torchvision.transforms as T\nfrom torchvision.models import resnet50\nfrom PIL import Image\nimport pandas as pd\nfrom tqdm import tqdm\n\n\n# ---------------------------\n# 1. RESNET50 PRETRAINED\n# ---------------------------\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nmodel = resnet50(weights=\"IMAGENET1K_V1\")\nmodel = torch.nn.Sequential(*list(model.children())[:-1])  # b·ªè FC ‚Üí embedding 2048D\nmodel.to(device)\nmodel.eval()\n\n# ---------------------------\n# 2. IMAGE TRANSFORM\n# ---------------------------\ntransform = T.Compose([\n    T.Resize((224, 224)),\n    T.ToTensor(),\n    T.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225],\n    ),\n])\n\n\n# ---------------------------\n# 3. EXTRACT FEATURE FROM 1 IMAGE\n# ---------------------------\ndef extract_deep_feature(img_path):\n    img = Image.open(img_path).convert(\"RGB\")\n    x = transform(img).unsqueeze(0).to(device)\n    with torch.no_grad():\n        feat = model(x).squeeze().cpu().numpy()\n    return feat\n\n\n# ---------------------------\n# 4. MAIN EXTRACTION LOOP\n# ---------------------------\ndef extract_from_dataset(base_dir,x):\n\n    rows = []\n    splits = [\"T1\",\"T2\"]\n\n    for split in splits:\n        img_root = os.path.join(base_dir, f\"{split}_images\")\n\n        if not os.path.exists(img_root):\n            print(f\"‚ùå Folder kh√¥ng t·ªìn t·∫°i: {img_root}\")\n            continue\n\n        # m·ªói folder b√™n trong = 1 b·ªánh nh√¢n\n        patients = [p for p in os.listdir(img_root) \n                    if os.path.isdir(os.path.join(img_root, p))]\n\n        for patient_id in tqdm(patients, desc=f\"Split {split}\"):\n\n            patient_folder = os.path.join(img_root, patient_id)\n\n            image_files = [\n                f for f in os.listdir(patient_folder)\n                if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n            ]\n\n            for img_file in image_files:\n\n                img_path = os.path.join(patient_folder, img_file)\n\n                feat = extract_deep_feature(img_path)\n\n                rows.append({\n                    \"Split\": split,\n                    \"PatientID\": patient_id,\n                    \"ImageFile\": img_file,\n                    **{f\"F{i}\": feat[i] for i in range(len(feat))}\n                })\n\n    df = pd.DataFrame(rows)\n    df.to_csv(f\"deep_features_2d_{x}.csv\", index=False)\n    print(f\"‚úÖ Saved deep_features_2d_{x}.csv\")\n\n    return df\n\n\n# RUN:\ndf1 = extract_from_dataset(\"/kaggle/input/healthy-data/Healthy_subjects/T1_W_Healthy\",11)\ndf2 = extract_from_dataset(\"/kaggle/input/healthy-data/Healthy_subjects/T2_W_Healthy\",12)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T11:10:53.095255Z","iopub.execute_input":"2025-12-03T11:10:53.095664Z","iopub.status.idle":"2025-12-03T11:13:55.020934Z","shell.execute_reply.started":"2025-12-03T11:10:53.095619Z","shell.execute_reply":"2025-12-03T11:13:55.020207Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# Load file deep feature c·ªßa b·∫°n\n# L·∫•y danh s√°ch c·ªôt feature F0..F2047\nfeature_cols = [c for c in df1.columns if c.startswith(\"F\")]\n\n# Group theo b·ªánh nh√¢n\npatient_df = (\n    df1.groupby([\"Split\", \"PatientID\"])[feature_cols]\n      .mean()     # mean pooling\n      .reset_index()\n)\n\n# L∆∞u file\npatient_df.to_csv(\"deep_features_patient_T1_healthy.csv\", index=False)\n\nprint(\"‚úÖ Saved deep_features_patient_T1.csv\")\nprint(\"S·ªë b·ªánh nh√¢n:\", len(patient_df))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T11:13:55.022072Z","iopub.execute_input":"2025-12-03T11:13:55.022326Z","iopub.status.idle":"2025-12-03T11:13:55.219633Z","shell.execute_reply.started":"2025-12-03T11:13:55.022302Z","shell.execute_reply":"2025-12-03T11:13:55.219042Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# Load file deep feature c·ªßa b·∫°n\ndf=df2\n# L·∫•y danh s√°ch c·ªôt feature F0..F2047\nfeature_cols = [c for c in df.columns if c.startswith(\"F\")]\n\n# Group theo b·ªánh nh√¢n\npatient_df = (\n    df.groupby([\"Split\", \"PatientID\"])[feature_cols]\n      .mean()     # mean pooling\n      .reset_index()\n)\n\n# L∆∞u file\npatient_df.to_csv(\"deep_features_patient_T2_healthy.csv\", index=False)\n\nprint(\"‚úÖ Saved deep_features_patient_T2.csv\")\nprint(\"S·ªë b·ªánh nh√¢n:\", len(patient_df))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T11:13:55.220299Z","iopub.execute_input":"2025-12-03T11:13:55.220593Z","iopub.status.idle":"2025-12-03T11:13:55.401997Z","shell.execute_reply.started":"2025-12-03T11:13:55.220574Z","shell.execute_reply":"2025-12-03T11:13:55.401418Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# N√©n folder working th√†nh zip\n!zip -r results.zip /kaggle/working/\n\n# T·∫°o link t·∫£i\nfrom IPython.display import FileLink\nFileLink(\"results.zip\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T11:13:55.403017Z","iopub.execute_input":"2025-12-03T11:13:55.403219Z","iopub.status.idle":"2025-12-03T11:16:05.277017Z","shell.execute_reply.started":"2025-12-03T11:13:55.403203Z","shell.execute_reply":"2025-12-03T11:16:05.276268Z"}},"outputs":[],"execution_count":null}]}